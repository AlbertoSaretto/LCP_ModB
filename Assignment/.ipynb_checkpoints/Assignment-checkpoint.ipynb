{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8f871d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import exp, sqrt, abs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "plt.rcParams['font.size'] = 14\n",
    "from IPython.display import display_html \n",
    "import pandas as pd\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f29ae9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sets(data, train):\n",
    "\n",
    "    # shuffle the data\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    # calculate the number of samples for the training set\n",
    "    N = data.shape[0]\n",
    "    Ntrain = int(N * train)\n",
    "\n",
    "    training = data[0:Ntrain]\n",
    "    validation = data[Ntrain:]\n",
    "\n",
    "    return training, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1997f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dname=\"DATA_b\"\n",
    "\n",
    "fname=dname+'/x_RBM_q0.1.dat'\n",
    "\n",
    "fname_test = dname+'/x_test_RBM_q0.1.dat'\n",
    "\n",
    "# define the relative dimension of the training and validation set\n",
    "perc_train = 0.70\n",
    "\n",
    "data = np.loadtxt(fname, delimiter=\" \",dtype=int)\n",
    "\n",
    "v, v_t = create_sets(data, perc_train)\n",
    "\n",
    "# loading data: each row is a list of visible units\n",
    "# NOTE: data \"x\" here is named \"v\" for \"visible\"\n",
    "#v = np.loadtxt(fname, delimiter=\" \",dtype=int)         # training set\n",
    "#v_t = np.loadtxt(fname_test, delimiter=\" \",dtype=int)  # test set\n",
    "\n",
    "# number of samples and length of a sequence\n",
    "N = len(v)\n",
    "N_t = len(v_t)\n",
    "L = len(v[1])\n",
    "\n",
    "#number of blocks in a sequence\n",
    "bn = 5\n",
    "\n",
    "#length of a block\n",
    "bl = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b6909dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBM, nr of hidden units\n",
    "M = 6\n",
    "\n",
    "# range of each initial weight: standard deviation\n",
    "sigma = sqrt(4. / float(L + M)) #L= number visible units, M= number of hidden units\n",
    "\n",
    "# random seed for reproducibility\n",
    "np.random.seed(12345)\n",
    "\n",
    "# initial weights from a Normal distr. (see literature, e.g. page 98 of Mehta's review)\n",
    "def init_w():\n",
    "    w = sigma * np.random.randn(L,M)\n",
    "    a = sigma * np.random.randn(L) #bias of visible units\n",
    "    b = np.zeros(M) #bias of hidden units\n",
    "    return w,a,b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bd126f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coord(np,x0,f=1.0):\n",
    "    x=[x0] * np\n",
    "    y=list(range(np))\n",
    "    for i in range(np):\n",
    "        y[i] = f*(y[i]/(np-1.) - 0.5)\n",
    "    return (x,y)\n",
    "\n",
    "(x1,y1)=create_coord(L,0)\n",
    "(x2,y2)=create_coord(M,1,f=0.7)\n",
    "\n",
    "def mycolor(val):\n",
    "    if val>0: return 'red'\n",
    "    elif val<0: return 'blue'\n",
    "    else: return 'black'\n",
    "\n",
    "def plotgraph_vert(epoch=0):\n",
    "    A=2./w.max()\n",
    "    for i in range(L):\n",
    "        for j in range(M):\n",
    "            ex, ey, col = (x1[i],x2[j]),(y1[i],y2[j]),mycolor(w[i][j])\n",
    "            plt.plot(ex, ey, col, zorder=1, lw=A*abs(w[i][j])) #per plottare le linee\n",
    "    \n",
    "    # Scatter plot on top of lines\n",
    "    A=300./(a.max()+b.max())\n",
    "    for i in range(L):\n",
    "        plt.scatter(x1[i], y1[i], s=A*abs(a[i]), zorder=2, c=mycolor(a[i])) #size Ã¨ proporz al bias \n",
    "\n",
    "    for j in range(M):\n",
    "        plt.scatter(x2[j], y2[j], s=A*abs(b[j]), zorder=2, c=mycolor(b[j]), marker=\"s\")\n",
    "\n",
    "    plt.figaspect(1)\n",
    "    plt.title(f'>0 red, <0 blue, epoch={epoch}')\n",
    "    plt.show()\n",
    "    \n",
    "def plotgraph(epoch=0):\n",
    "    fig, ax = plt.subplots(1,1 , figsize=(10, 5))\n",
    "    ax.tick_params(left=False,bottom=False)\n",
    "    ax.xaxis.set_major_formatter(NullFormatter())\n",
    "    ax.yaxis.set_major_formatter(NullFormatter())\n",
    "    \n",
    "    A=1./max(w.max(),-w.min())\n",
    "    for i in range(L):\n",
    "        for j in range(M):\n",
    "            ex, ey, col = (y1[i],y2[j]),(x1[i],x2[j]),mycolor(w[i][j])\n",
    "            ax.plot(ex, ey, col, zorder=1, alpha=A*abs(w[i][j]))\n",
    "    \n",
    "    # Scatter plot on top of lines\n",
    "    #A=300./(a.max()+b.max())\n",
    "    A=500.\n",
    "    for i in range(L):\n",
    "        ax.scatter(y1[i],x1[i], s=A*abs(a[i]), zorder=2, c=mycolor(a[i]))\n",
    "    for j in range(M):\n",
    "        ax.scatter(y2[j], x2[j], s=min(300,A*abs(b[j])), zorder=2, c=mycolor(b[j]), marker=\"s\")\n",
    "    ax.set_title(f'>0 red, <0 blue, epoch={epoch}')\n",
    "    ax.text(-0.5,0.9,\"hidden\\nlayer\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f149153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eq(213) page 97, activation via sigmoid\n",
    "# taking into account energy gap DE=2 for \"spin\" variables (-1,1)\n",
    "def activate(v_in,wei,bias,DE,info=False):\n",
    "    act = np.dot(v_in, wei) + bias \n",
    "    n = np.shape(act)\n",
    "    prob = 1. / (1. + exp(-DE*act)) #prob for sigmoid\n",
    "    v_out = np.full(n, vmin, dtype=int) # a list on -1's or 0's: initialize initial vector (vim= 0 (bits) or -1 (spins))\n",
    "    v_out[np.random.random_sample(n) < prob] = 1 # activate the 1's with probability prob\n",
    "    \n",
    "    if info:\n",
    "        print('input=', v_in)\n",
    "        print('act=',act)\n",
    "        print('prob=',prob)\n",
    "        print('output=',v_out)\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b87bdf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to calculate <E> - lnZ, average of E on data and of Z on all h's and v's\n",
    "def partition(w, a, b, bl, bn, SPINS):\n",
    "\n",
    "    vl = a.shape[0] #20 here\n",
    "    hl = b.shape[0] #6 here\n",
    "\n",
    "    binary = np.array([0, 1])\n",
    "\n",
    "    # create a matrix whose rows make up a list of all the possible combinations of 0,1\n",
    "    #2**6 rows, 6 cols here\n",
    "    h = np.indices((2,) * hl).reshape((hl, -1)).T\n",
    "\n",
    "    # create a matrix whose rows make up a list of all the possible combinations of 0,1,2,3,..., bl\n",
    "    #4**5 rows, 20 cols here\n",
    "    v = np.indices((bl,) * bn).reshape((bn, -1)).T\n",
    "\n",
    "    # now map it to the binary version\n",
    "    mapping = np.eye(bl)\n",
    "    mapping = mapping[::-1] # each row is the binary number corresponding to the (decimal) row's index + 1\n",
    "    v = mapping[v]\n",
    "    v = v.reshape((v.shape[0],v.shape[1] * v.shape[2]))\n",
    "\n",
    "    if SPINS:\n",
    "        # convert 0,1 -> -1,1\n",
    "        v = 2*v - 1\n",
    "    \n",
    "    #calculate partition function:\n",
    "    #is the summation on all possible values of visible and hidden layer \n",
    "    Z=0\n",
    "\n",
    "    for i in range(v.shape[0]):\n",
    "        for j in range(h.shape[0]):\n",
    "\n",
    "            Z += np.exp(-energy(v[i], h[j], a, b, w))\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7984c603",
   "metadata": {},
   "source": [
    "## Contrastive divergence: preserving one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bcf529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tolto np.sum davanti ai primi due np.dot\n",
    "#function energy\n",
    "\n",
    "def energy(v, h, a, b, w):\n",
    "\n",
    "    return -(np.dot(v,a) + np.dot(h,b) + np.dot(v, np.dot(w,h)))\n",
    "\n",
    "\n",
    "#function fantasy in order to generate fantasy data from hidden variables\n",
    "def fantasy(h, w, a, b, DE, SPINS):\n",
    "\n",
    "    Z=0\n",
    "    bn = 5\n",
    "    bl = 4\n",
    "\n",
    "    beta = 1\n",
    "\n",
    "    v = np.zeros(bn*bl)\n",
    "\n",
    "    for block in range(bn):\n",
    "        \n",
    "        vs = np.diag((1,1,1,1))\n",
    "\n",
    "        if SPINS: \n",
    "\n",
    "            GAP=2\n",
    "            vs = 2*vs - 1\n",
    "\n",
    "        E = np.zeros(bl)\n",
    "        prob = np.zeros(bl)  \n",
    "\n",
    "        a_red = a[block*bl:(block+1)*bl]       \n",
    "        w_red = w[block*bl:(block+1)*bl,:]\n",
    "\n",
    "\n",
    "        for i in range(bl): E[i] = energy(vs[i], h, a_red, b, w_red)\n",
    "        \n",
    "        prob = np.exp(-beta * E)\n",
    "\n",
    "        # normalisation\n",
    "        Z = np.cumsum(prob)[-1]\n",
    "        prob = prob/Z\n",
    "\n",
    "        v[block*bl:(block+1)*bl] = vs[np.random.choice(4, p=prob)]\n",
    "\n",
    "    #print(prob)\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99832ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglike(v_t, N_t, w, a, b, bl, bn, GAP, SPINS):\n",
    "\n",
    "    ampl=40\n",
    "    E=0\n",
    "\n",
    "    for k in range(N_t):\n",
    "\n",
    "        # positive CD phase: generating h \n",
    "        h = activate(v_t[k],w,b,GAP*ampl)\n",
    "        \n",
    "        #sum energy for every visible (and hidden) layer in dataset\n",
    "        E += energy(v_t[k],h,a,b,w)\n",
    "\n",
    "    #mean energy at the end of the epoch\n",
    "    E /= N_t\n",
    "\n",
    "    #evaluate the partition function with parameters from the training on the epoch\n",
    "    Z = partition(w,a,b,bl,bn, SPINS=SPINS)\n",
    "\n",
    "    #evaluate log likelihood as - <E> - lnZ\n",
    "    LL = - E - np.log(Z)\n",
    "\n",
    "    return LL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417c78f5",
   "metadata": {},
   "source": [
    "## Comparison between spin={0,1} and spin={-1,1}\n",
    "\n",
    "### Spin={-1,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e961434",
   "metadata": {},
   "outputs": [],
   "source": [
    "v, v_t = create_sets(data, perc_train)\n",
    "\n",
    "w,a,b=init_w()\n",
    "\n",
    "SPINS = True\n",
    "\n",
    "if SPINS:\n",
    "    # sigmoid takes into account energy difference =2\n",
    "    GAP=2\n",
    "    # convert 0,1 -> -1,1\n",
    "    v = 2*v - 1\n",
    "    vmin=-1\n",
    "else:\n",
    "    GAP=1\n",
    "    vmin=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a30f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed for reproducibility\n",
    "np.random.seed(12345)\n",
    "\n",
    "plotgraph(0)\n",
    "\n",
    "# learning rate\n",
    "l_rate = 1.0\n",
    "\n",
    "# minibatch\n",
    "mini, m = 500, 0\n",
    "\n",
    "LL_val_t=np.zeros(100)\n",
    "\n",
    "# train model\n",
    "print('===================================================')\n",
    "for epoch in range(100):\n",
    "    # aggregate normalization of batch statistics and learning rate\n",
    "    l_rate_m = l_rate / (5*mini)\n",
    "    for k in range(N):\n",
    "        if m==0:\n",
    "            # initialize averages in miniblock\n",
    "            v_data, v_model = np.zeros(L),np.zeros(L)\n",
    "            h_data, h_model = np.zeros(M),np.zeros(M)\n",
    "            vh_data,vh_model= np.zeros((L,M)),np.zeros((L,M))\n",
    "\n",
    "        #need to intoduce a for cycle in exercises\n",
    "        # positive CD phase: generating h \n",
    "        h = activate(v[k],w,b,GAP)\n",
    "        # negative CD phase: generating fantasy vf\n",
    "        vf = fantasy(h,w,a,b,GAP, SPINS = True)  #need to change h to hf??\n",
    "        # one more positive CD phase: generating fantasy h from fantasy vf \n",
    "        hf = activate(vf,w,b,GAP)\n",
    "\n",
    "        v_data  += v[k]\n",
    "        v_model += vf\n",
    "        h_data  += h\n",
    "        h_model += hf\n",
    "        vh_data += np.outer(v[k].T,h)\n",
    "        vh_model+= np.outer(vf.T,hf)\n",
    "\n",
    "        #if k==0: print(vf)\n",
    "\n",
    "        m += 1\n",
    "        # minibatch\n",
    "        if m==mini:\n",
    "            # gradient of the likelihood: follow it along its positive direction\n",
    "            # with a \"vanilla\" SGD\n",
    "            dw = l_rate_m*(vh_data - vh_model) \n",
    "            da = l_rate_m*(v_data - v_model)\n",
    "            db = l_rate_m*(h_data - h_model)\n",
    "\n",
    "            # basic step of vanilla gradient descent, from eq.(211)\n",
    "            w = w + dw\n",
    "            a = a + da\n",
    "            b = b + db\n",
    "            m=0\n",
    "    \n",
    "    LL_val_t[epoch] = loglike(v_t, N_t, w, a, b, bl, bn, GAP, SPINS)\n",
    "    print(loglike(v_t, N_t, w, a, b, bl, bn, GAP, SPINS))\n",
    "\n",
    "    # randomize the order of input data\n",
    "    np.random.shuffle(v)\n",
    "    # decrease the learning rate (here as a power law)\n",
    "    l_rate = l_rate / (0.01 * l_rate + 1)\n",
    "\n",
    "    \n",
    "    \n",
    "    if epoch==99:\n",
    "        plotgraph(epoch+1)\n",
    "        print('l_rate = ',l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1452df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot result for log-likelihood\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,6),nrows=1, ncols=1)\n",
    "\n",
    "ax.plot((np.arange(100)+1), LL_val_t)\n",
    "\n",
    "print(\"log-likelihood: \", LL_val_t[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f73105",
   "metadata": {},
   "source": [
    "### Spin= {0,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1727d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reassign data and weights and biases\n",
    "\n",
    "v, v_t = create_sets(np.loadtxt(fname, delimiter=\" \",dtype=int), perc_train)\n",
    "\n",
    "w,a,b=init_w()\n",
    "\n",
    "SPINS = False\n",
    "\n",
    "if SPINS:\n",
    "    # sigmoid takes into account energy difference =2\n",
    "    GAP=2\n",
    "    # convert 0,1 -> -1,1\n",
    "    v = 2*v - 1\n",
    "    vmin=-1\n",
    "else:\n",
    "    GAP=1\n",
    "    vmin=0\n",
    "\n",
    "#training RBM\n",
    "\n",
    "plotgraph(0)\n",
    "\n",
    "# learning rate\n",
    "l_rate = 1.0\n",
    "\n",
    "# minibatch\n",
    "mini, m = 500, 0\n",
    "\n",
    "LL_val_f=np.zeros(100)\n",
    "\n",
    "# train model\n",
    "print('===================================================')\n",
    "for epoch in range(100):\n",
    "    # aggregate normalization of batch statistics and learning rate\n",
    "    #ABBASSARE LR\n",
    "    l_rate_m = l_rate / (10*mini)\n",
    "    for k in range(N):\n",
    "        if m==0:\n",
    "            # initialize averages in miniblock\n",
    "            v_data, v_model = np.zeros(L),np.zeros(L)\n",
    "            h_data, h_model = np.zeros(M),np.zeros(M)\n",
    "            vh_data,vh_model= np.zeros((L,M)),np.zeros((L,M))\n",
    "\n",
    "        #need to intoduce a for cycle in exercises\n",
    "        # positive CD phase: generating h \n",
    "        h = activate(v[k],w,b,GAP)\n",
    "        # negative CD phase: generating fantasy vf\n",
    "        vf = fantasy(h,w,a,b,GAP, SPINS = False)   #need to change h to hf??\n",
    "        # one more positive CD phase: generating fantasy h from fantasy vf \n",
    "        hf = activate(vf,w,b,GAP)\n",
    "\n",
    "        v_data  += v[k]\n",
    "        v_model += vf\n",
    "        h_data  += h\n",
    "        h_model += hf\n",
    "        vh_data += np.outer(v[k].T,h)\n",
    "        vh_model+= np.outer(vf.T,hf)\n",
    "\n",
    "        #if epoch==99: print(vf)\n",
    "\n",
    "        m += 1\n",
    "        # minibatch\n",
    "        if m==mini:\n",
    "            # gradient of the likelihood: follow it along its positive direction\n",
    "            # with a \"vanilla\" SGD\n",
    "            dw = l_rate_m*(vh_data - vh_model) \n",
    "            da = l_rate_m*(v_data - v_model)\n",
    "            db = l_rate_m*(h_data - h_model)\n",
    "\n",
    "            # basic step of vanilla gradient descent, from eq.(211)\n",
    "            w = w + dw\n",
    "            a = a + da\n",
    "            b = b + db\n",
    "            m=0\n",
    "\n",
    "\n",
    "    LL_val_f[epoch] = loglike(v_t, N_t, w, a, b, bl, bn, GAP, SPINS)\n",
    "    \n",
    "    # randomize the order of input data\n",
    "    np.random.shuffle(v)\n",
    "    # decrease the learning rate (here as a power law)\n",
    "    l_rate = l_rate / (0.01 * l_rate + 1)\n",
    "    if epoch==99:\n",
    "        plotgraph(epoch+1)\n",
    "        print('l_rate = ',l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61c7143",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18,6),nrows=1, ncols=1)\n",
    "\n",
    "#subLL_vals = [LL_val_f[i:i+10] for i in range(0, 100, 10)]  # divide l'array in 10 sottoarray di 10 elementi ciascuno\n",
    "\n",
    "#sums_LL = [sum(subLL_val) for subsubLL_val in subLL_vals]  # calcola la somma di ogni sottoarray\n",
    "\n",
    "#print(sums)\n",
    "\n",
    "ax.plot((np.arange(100)+1), LL_val_f)\n",
    "\n",
    "print(\"log-likelihood: \", LL_val_f[-1])\n",
    "\n",
    "print(\"log-likelihood\", loglike(v_t, N_t, w, a, b, bl, bn, GAP, SPINS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c551010",
   "metadata": {},
   "source": [
    "#### from now on we shall choose Spins = 0,1\n",
    "\n",
    "#### remember to initialize data and parameters at every new training, and set Spins = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a970b7",
   "metadata": {},
   "source": [
    "## Implementation of Adam gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4002a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reassign data and weights and biases\n",
    "\n",
    "v, v_t = create_sets(np.loadtxt(fname, delimiter=\" \",dtype=int), perc_train)\n",
    "\n",
    "w,a,b=init_w()\n",
    "\n",
    "SPINS = True\n",
    "\n",
    "if SPINS:\n",
    "    # sigmoid takes into account energy difference =2\n",
    "    GAP=2\n",
    "    # convert 0,1 -> -1,1\n",
    "    v = 2*v - 1\n",
    "    vmin=-1\n",
    "else:\n",
    "    GAP=1\n",
    "    vmin=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotgraph(0)\n",
    "#print(a,b,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0856be22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "l_rate = 1.0\n",
    "\n",
    "# minibatch\n",
    "mini, m = N-1, 0\n",
    "\n",
    "\n",
    "###### ADAM PARAMETERS #######\n",
    "steps = 0\n",
    "\n",
    "beta1, beta2, epsilon = 0.9, 0.999, 1e-8\n",
    "\n",
    "#initialize first and second moments estimates\n",
    "m_w, s_w = np.zeros_like(w), np.zeros_like(w)\n",
    "m_a, s_a = np.zeros_like(a), np.zeros_like(a)\n",
    "m_b, s_b = np.zeros_like(b), np.zeros_like(b)\n",
    "\n",
    "##############################\n",
    "\n",
    "#occhio cambiato segno nell'aggiornamento dei pesi\n",
    "\n",
    "\n",
    "plotgraph(0)\n",
    "\n",
    "LL_train, LL_val = np.zeros(100), np.zeros(100)\n",
    "\n",
    "# train model\n",
    "print('===================================================')\n",
    "for epoch in range(100):\n",
    "    # aggregate normalization of batch statistics and learning rate\n",
    "    #forse alzare\n",
    "    l_rate_m = l_rate / 5\n",
    "    \n",
    "    for k in range(N):\n",
    "        if m==0:\n",
    "            # initialize averages in miniblock\n",
    "            v_data, v_model = np.zeros(L),np.zeros(L)\n",
    "            h_data, h_model = np.zeros(M),np.zeros(M)\n",
    "            vh_data,vh_model= np.zeros((L,M)),np.zeros((L,M))\n",
    "\n",
    "        # CD-1\n",
    "        # positive CD phase: generating h \n",
    "        h = activate(v[k],w,b,GAP)\n",
    "        # negative CD phase: generating fantasy vf\n",
    "        vf = fantasy(h,w,a,b,GAP, SPINS)  \n",
    "        # one more positive CD phase: generating fantasy h from fantasy vf \n",
    "        hf = activate(vf,w,b,GAP)\n",
    "\n",
    "        v_data  += v[k]\n",
    "        v_model += vf\n",
    "        h_data  += h\n",
    "        h_model += hf\n",
    "        vh_data += np.outer(v[k].T,h)\n",
    "        vh_model+= np.outer(vf.T,hf)\n",
    "\n",
    "#        if k==N/2: print(prob)\n",
    "\n",
    "        m += 1\n",
    "        # minibatch\n",
    "        if m==mini:\n",
    "        #if k==N-1:\n",
    "            # gradient of the likelihood: follow it along its positive direction\n",
    "            grad_w = (vh_data - vh_model) / mini\n",
    "            grad_a = (v_data - v_model) / mini\n",
    "            grad_b = (h_data - h_model) / mini\n",
    "\n",
    "            # Update the first moment estimates\n",
    "            m_w = beta1 * m_w + (1 - beta1) * grad_w\n",
    "            m_a = beta1 * m_a + (1 - beta1) * grad_a\n",
    "            m_b = beta1 * m_b + (1 - beta1) * grad_b\n",
    "\n",
    "            # Update the second moment estimates\n",
    "            s_w = beta2 * s_w + (1 - beta2) * grad_w**2\n",
    "            s_a = beta2 * s_a + (1 - beta2) * grad_a**2\n",
    "            s_b = beta2 * s_b + (1 - beta2) * grad_b**2\n",
    "\n",
    "            # Compute the bias-corrected first and second moment estimates\n",
    "            #changed epoch to steps to account for number of iteration of Adam alg\n",
    "            \n",
    "            m_w_hat = m_w / (1 - beta1**(steps+1))\n",
    "            m_a_hat = m_a / (1 - beta1**(steps+1))\n",
    "            m_b_hat = m_b / (1 - beta1**(steps+1))\n",
    "            s_w_hat = s_w / (1 - beta2**(steps+1))\n",
    "            s_a_hat = s_a / (1 - beta2**(steps+1))\n",
    "            s_b_hat = s_b / (1 - beta2**(steps+1))\n",
    "\n",
    "            # Update the weights and biases using Adam update rule\n",
    "                        \n",
    "            w = w + l_rate_m * m_w_hat / (np.sqrt(s_w_hat) + epsilon)\n",
    "            a = a + l_rate_m * m_a_hat / (np.sqrt(s_a_hat) + epsilon)\n",
    "            b = b + l_rate_m * m_b_hat / (np.sqrt(s_b_hat) + epsilon)\n",
    "\n",
    "            m=0\n",
    "\n",
    "    LL_train[epoch] = loglike(v, N, w, a, b, bl, bn, GAP, SPINS)\n",
    "    LL_val[epoch] = loglike(v_t, N_t, w, a, b, bl, bn, GAP, SPINS)\n",
    "\n",
    "    print(loglike(v,N,w,a,b,bl,bn,GAP,SPINS))\n",
    "        \n",
    "    steps += 1\n",
    "            \n",
    "    # randomize the order of input data\n",
    "    np.random.shuffle(v)\n",
    "    # decrease the learning rate (here as a power law)\n",
    "    l_rate = l_rate / (0.01 * l_rate + 1)\n",
    "    if epoch==99:\n",
    "       plotgraph(epoch+1)\n",
    "       print('l_rate = ',l_rate)\n",
    "       print(\"number of total steps: \", steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bd5b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18,6),nrows=1, ncols=1)\n",
    "\n",
    "ax.plot((np.arange(100)+1), LL_val)\n",
    "\n",
    "print(\"log-likelihood: \\n\", loglike(v_t, N_t, w, a, b, bl, bn, GAP, SPINS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4563455f",
   "metadata": {},
   "source": [
    "## Increase the number of contrastive divergence steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51b8beaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reassign data and weights and biases\n",
    "v, v_t = create_sets(data, perc_train)\n",
    "\n",
    "w,a,b=init_w()\n",
    "\n",
    "SPINS = False\n",
    "\n",
    "if SPINS:\n",
    "    # sigmoid takes into account energy difference =2\n",
    "    GAP=2\n",
    "    # convert 0,1 -> -1,1\n",
    "    v = 2*v - 1\n",
    "    vmin=-1\n",
    "else:\n",
    "    GAP=1\n",
    "    vmin=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec14c794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "fantasy() missing 1 required positional argument: 'SPINS'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m hf \u001b[38;5;241m=\u001b[39m h\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# negative CD phase: generating fantasy vf\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     vf \u001b[38;5;241m=\u001b[39m \u001b[43mfantasy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mGAP\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# positive CD phase: generating fantasy h from fantasy vf \u001b[39;00m\n\u001b[0;32m     59\u001b[0m     hf \u001b[38;5;241m=\u001b[39m activate(vf,w,b,GAP)\n",
      "\u001b[1;31mTypeError\u001b[0m: fantasy() missing 1 required positional argument: 'SPINS'"
     ]
    }
   ],
   "source": [
    "# random seed for reproducibility\n",
    "np.random.seed(12345)\n",
    "    \n",
    "\n",
    "# learning rate\n",
    "l_rate = 1.0\n",
    "\n",
    "# minibatch\n",
    "mini, m = 500, 0\n",
    "\n",
    "##############################\n",
    "\n",
    "# train model\n",
    "print('===================================================')\n",
    "\n",
    "LL_list = np.zeros(10)\n",
    "\n",
    "for k in range(10):  #Increase the number of CD steps\n",
    "\n",
    "    #Reinitialize data before training\n",
    "\n",
    "    v, v_t = create_sets(data, perc_train)\n",
    "\n",
    "    w,a,b=init_w()\n",
    "\n",
    "    SPINS = False\n",
    "\n",
    "    GAP = 1\n",
    "    vmin = 0\n",
    "\n",
    "    ###### ADAM PARAMETERS #######\n",
    "\n",
    "    beta1, beta2, epsilon = 0.9, 0.99, 1e-8\n",
    "\n",
    "    #initialize first and second moments estimates\n",
    "    m_w, s_w = np.zeros_like(w), np.zeros_like(w)\n",
    "    m_a, s_a = np.zeros_like(a), np.zeros_like(a)\n",
    "    m_b, s_b = np.zeros_like(b), np.zeros_like(b)\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(100):\n",
    "        # aggregate normalization of batch statistics and learning rate\n",
    "        l_rate_m = l_rate / mini\n",
    "        for i in range(N):\n",
    "            if m==0:\n",
    "                # initialize averages in miniblock\n",
    "                v_data, v_model = np.zeros(L),np.zeros(L)\n",
    "                h_data, h_model = np.zeros(M),np.zeros(M)\n",
    "                vh_data,vh_model= np.zeros((L,M)),np.zeros((L,M))\n",
    "\n",
    "            # positive CD phase: generating h \n",
    "            h = activate(v[i],w,b,GAP)\n",
    "            hf = h\n",
    "            for j in range(k+1):\n",
    "                # negative CD phase: generating fantasy vf\n",
    "                vf = fantasy(hf,w,a,b,GAP, SPINS) \n",
    "                # positive CD phase: generating fantasy h from fantasy vf \n",
    "                hf = activate(vf,w,b,GAP, SPINS)\n",
    "\n",
    "            v_data  += v[i]\n",
    "            v_model += vf\n",
    "            h_data  += h\n",
    "            h_model += hf\n",
    "            vh_data += np.outer(v[i].T,h)\n",
    "            vh_model+= np.outer(vf.T,hf)\n",
    "        \n",
    "            m += 1\n",
    "            # minibatch\n",
    "            if m==mini:\n",
    "                \n",
    "                ##### ADAM ####\n",
    "                \n",
    "                # gradient of the likelihood: follow it along its positive direction\n",
    "                grad_w = (vh_data - vh_model) / mini\n",
    "                grad_a = (v_data - v_model) / mini\n",
    "                grad_b = (h_data - h_model) / mini\n",
    "\n",
    "                # Update the first moment estimates\n",
    "                m_w = beta1 * m_w + (1 - beta1) * grad_w\n",
    "                m_a = beta1 * m_a + (1 - beta1) * grad_a\n",
    "                m_b = beta1 * m_b + (1 - beta1) * grad_b\n",
    "\n",
    "                # Update the second moment estimates\n",
    "                s_w = beta2 * s_w + (1 - beta2) * grad_w**2\n",
    "                s_a = beta2 * s_a + (1 - beta2) * grad_a**2\n",
    "                s_b = beta2 * s_b + (1 - beta2) * grad_b**2\n",
    "\n",
    "                # Compute the bias-corrected first and second moment estimates\n",
    "                m_w_hat = m_w / (1 - beta1**(epoch+1))\n",
    "                m_a_hat = m_a / (1 - beta1**(epoch+1))\n",
    "                m_b_hat = m_b / (1 - beta1**(epoch+1))\n",
    "                s_w_hat = s_w / (1 - beta2**(epoch+1))\n",
    "                s_a_hat = s_a / (1 - beta2**(epoch+1))\n",
    "                s_b_hat = s_b / (1 - beta2**(epoch+1))\n",
    "\n",
    "                # Update the weights and biases using Adam update rule\n",
    "                w = w + l_rate_m * m_w_hat / (np.sqrt(s_w_hat) + epsilon)\n",
    "                a = a + l_rate_m * m_a_hat / (np.sqrt(s_a_hat) + epsilon)\n",
    "                b = b + l_rate_m * m_b_hat / (np.sqrt(s_b_hat) + epsilon)\n",
    "\n",
    "                m=0\n",
    "        \n",
    "        # randomize the order of input data\n",
    "        np.random.shuffle(v)\n",
    "        \n",
    "        # decrease the learning rate (here as a power law)\n",
    "        l_rate = l_rate / (0.01 * l_rate + 1)\n",
    "\n",
    "\n",
    "\n",
    "        if epoch==99:\n",
    "            plotgraph(epoch+1)\n",
    "            print('l_rate = ',l_rate)\n",
    "\n",
    "    LL_list[k] = loglike(v_t, N_t, w, a, b, bl, bn, GAP, SPINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fca36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot result for multiple contrastive divergence \n",
    "\n",
    "fig, ax = plt.subplot(figsize=(12,6))\n",
    "\n",
    "ax.plot((np.arange(10)+1), LL_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69545274",
   "metadata": {},
   "source": [
    "## Implementing the centering trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6ed5c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
